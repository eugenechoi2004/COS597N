{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1dbfb0",
   "metadata": {},
   "source": [
    "# COS597N Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfe17c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/byw2/.conda/envs/dr-bert-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizerFast, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, log_loss, accuracy_score, matthews_corrcoef\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f851d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pickle.load(open('../Datasets/train.pkl', \"rb\"))\n",
    "df_val = pickle.load(open('../Datasets/val.pkl', \"rb\"))\n",
    "df_test = pickle.load(open('../Datasets/caid.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a5f054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset: (1569, 4)\n",
      "Validation Dataset: (176, 4)\n",
      "Test Dataset: (652, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Dataset: {df_train.shape}')\n",
    "print(f'Validation Dataset: {df_val.shape}')\n",
    "print(f'Test Dataset: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75199f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sequence', 'full', 'disprot_ID', 'intersect'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e7ee97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>full</th>\n",
       "      <th>disprot_ID</th>\n",
       "      <th>intersect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MELITNELLYKTYKQKPVGVEEPVYDQAGDPLFGERGAVHPQSTLK...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>DP00675</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MEDINFASLAPRHGSRPFMGNWQDIGTSNMSGGAFSWGSLWSGIKN...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>DP00808</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MAAKFEVGSVYTGKVTGLQAYGAFVALDEETQGLVHISEVTHGFVK...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>DP00809</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VVYTDCTESGQNLCLCEGSNVCGQGNKCILGSDGEKNQCVTGEGTP...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>DP00137</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MNYIPTQTFYGRRWRPRPAARPWPLQATPVAPVVPDFQAQQMQQLI...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>DP00999</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sequence  \\\n",
       "0   MELITNELLYKTYKQKPVGVEEPVYDQAGDPLFGERGAVHPQSTLK...   \n",
       "5   MEDINFASLAPRHGSRPFMGNWQDIGTSNMSGGAFSWGSLWSGIKN...   \n",
       "7   MAAKFEVGSVYTGKVTGLQAYGAFVALDEETQGLVHISEVTHGFVK...   \n",
       "9   VVYTDCTESGQNLCLCEGSNVCGQGNKCILGSDGEKNQCVTGEGTP...   \n",
       "11  MNYIPTQTFYGRRWRPRPAARPWPLQATPVAPVVPDFQAQQMQQLI...   \n",
       "\n",
       "                                                 full disprot_ID  intersect  \n",
       "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    DP00675      False  \n",
       "5   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    DP00808      False  \n",
       "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    DP00809      False  \n",
       "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    DP00137      False  \n",
       "11  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    DP00999      False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f015d49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>full</th>\n",
       "      <th>disprot_ID</th>\n",
       "      <th>intersect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>MIPVTILCVLLCLNLAWAQDGKTTFEKEGGGGRGPRILENMHESSC...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>DP00233</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>MGNLESTDGGPGEPPSVPLLLPPGKTPMPEPCELEERFALVLSSMN...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>DP02638</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>MSGSVSGCGSGGCSIVWFRRDLRVEDNPALAAAVRAGPVIALFVWA...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>DP00474</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>MATVNQLVRKPRARKVAKSNVPALEACPQKRGVCTRVYTTTPKKPN...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>DP00145</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>MAQWEMLQNLDSPFQDQLHQLYSHSLLPVDIRQYLAVWIEDQNWQE...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>DP00961</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sequence  \\\n",
       "1823  MIPVTILCVLLCLNLAWAQDGKTTFEKEGGGGRGPRILENMHESSC...   \n",
       "1283  MGNLESTDGGPGEPPSVPLLLPPGKTPMPEPCELEERFALVLSSMN...   \n",
       "1868  MSGSVSGCGSGGCSIVWFRRDLRVEDNPALAAAVRAGPVIALFVWA...   \n",
       "1416  MATVNQLVRKPRARKVAKSNVPALEACPQKRGVCTRVYTTTPKKPN...   \n",
       "170   MAQWEMLQNLDSPFQDQLHQLYSHSLLPVDIRQYLAVWIEDQNWQE...   \n",
       "\n",
       "                                                   full disprot_ID  intersect  \n",
       "1823  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    DP00233      False  \n",
       "1283  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    DP02638      False  \n",
       "1868  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...    DP00474      False  \n",
       "1416  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...    DP00145      False  \n",
       "170   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    DP00961      False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a37aa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DP00084</td>\n",
       "      <td>MSDNDDIEVESDEEQPRFQSAADKRAHHNALERKRRDHIKDSFHSL...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DP00182</td>\n",
       "      <td>MAPTKRKGSCPGAAPKKPKEPVQVPKLVIKGGIEVLGVKTGVDSFT...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DP00206</td>\n",
       "      <td>MKAAQKGFTLIELMIVVAIIGILAAIAIPAYQDYTARAQLSERMTL...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DP00334</td>\n",
       "      <td>MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DP00359</td>\n",
       "      <td>MMLTKSVVISRPAVRPVSTRRAVVVRASGQPAVDLNKKVQDAVKEA...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                           Sequence  \\\n",
       "0  DP00084  MSDNDDIEVESDEEQPRFQSAADKRAHHNALERKRRDHIKDSFHSL...   \n",
       "1  DP00182  MAPTKRKGSCPGAAPKKPKEPVQVPKLVIKGGIEVLGVKTGVDSFT...   \n",
       "2  DP00206  MKAAQKGFTLIELMIVVAIIGILAAIAIPAYQDYTARAQLSERMTL...   \n",
       "3  DP00334  MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...   \n",
       "4  DP00359  MMLTKSVVISRPAVRPVSTRRAVVVRASGQPAVDLNKKVQDAVKEA...   \n",
       "\n",
       "                                                full  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6cfef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_train, df_val, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032e181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     2397.000000\n",
       "mean       549.033792\n",
       "std        884.678317\n",
       "min         19.000000\n",
       "25%        204.000000\n",
       "50%        392.000000\n",
       "75%        651.000000\n",
       "max      34350.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_lens = pd.Series([len(seq) for seq in df_full['Sequence']])\n",
    "protein_lens.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2280b",
   "metadata": {},
   "source": [
    "Great, we can verify that none of the training and validation examples overlap with the test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3c2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-6\n",
    "BATCH_SIZE = 1\n",
    "TOKENIZER_PATH =  \"../checkpoint-final/\"\n",
    "# is this pretrained on protein sequences?\n",
    "PRETRAINED_MODEL = \"../checkpoint-final/\"\n",
    "NUM_CLASSES = 2\n",
    "SCHEDULER='cosine_with_restarts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63e9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDegreeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, max_length, df, tokenizer, region_type):\n",
    "        self.region_type = region_type # e.g. 'full'\n",
    "        self.df = df\n",
    "        self.seqs, self.labels = self.load_dataset()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_dataset(self):\n",
    "        seq = list(self.df['Sequence']) # list of protein sequences\n",
    "        label = list(self.df[self.region_type]) # list of list of labels\n",
    "        return seq, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
    "        seq = re.sub(r\"[UZOB]\", \"X\", seq)\n",
    "\n",
    "        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n",
    "        tens = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        sample['labels'] = F.pad(tens, (0, MAX_LENGTH - len(tens)))\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc73e76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a357d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProteinDegreeDataset(MAX_LENGTH, df_train, tokenizer, 'full')\n",
    "val_dataset = ProteinDegreeDataset(MAX_LENGTH, df_val, tokenizer, 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72bc7a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 4, 21,  9,  ..., 14,  5,  0]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       " 'labels': tensor([0, 0, 0,  ..., 0, 0, 0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "191c7ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     93\n",
       "15    83\n",
       "12    83\n",
       "7     82\n",
       "8     67\n",
       "9     63\n",
       "16    53\n",
       "11    52\n",
       "13    51\n",
       "6     48\n",
       "10    47\n",
       "14    45\n",
       "17    41\n",
       "18    40\n",
       "20    39\n",
       "23    37\n",
       "19    31\n",
       "21    23\n",
       "24    23\n",
       "22    21\n",
       "4      1\n",
       "0      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_dataset[0]['input_ids']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588e157",
   "metadata": {},
   "source": [
    "There are 25 tokens in the vocabulary (20 amino acids and [\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff99a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1024\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_dataset[0]['attention_mask']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c93618",
   "metadata": {},
   "source": [
    "All tokens should be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf275330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1024 1024\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset[0]['attention_mask']), len(train_dataset[0]['input_ids']), len(train_dataset[0]['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0f540",
   "metadata": {},
   "source": [
    "Finally, note that all training points are truncated to be 1024 amino acids for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "644b277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1_roc_convolve(name, logits, labels, convolution):\n",
    "    convolved = np.convolve(np.array(logits).flatten(), np.array(convolution / np.sum(convolution)).flatten(), 'same')\n",
    "    p = [(1 - i, i) for i in convolved]\n",
    "    roc = [i[1] for i in p]\n",
    "    roc2 = [i[0] for i in p]\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(labels, p)\n",
    "    roc_auc = roc_auc_score(labels, roc)\n",
    "    mcc = matthews_corrcoef(labels, p)\n",
    "    return {\n",
    "        f'precision_{name}':precision[1],\n",
    "        f'recall_{name}':recall[1],\n",
    "        f'f1_{name}':f1[1],\n",
    "        f'roc_auc_{name}':roc_auc,\n",
    "        f'mcc_{name}': mcc,\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    logits = softmax(logits, axis=2)\n",
    "    l = []\n",
    "    for j, i in enumerate(labels):\n",
    "        l = l + list(i[:len(df_val['Sequence'].iloc[j])])\n",
    "    lg2 = []\n",
    "    for k, i in enumerate(logits):\n",
    "        lg2 = lg2 + [j[1] for j in i[:len(df_val['Sequence'].iloc[k])]]\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics.update(precision_recall_f1_roc_convolve('normal', lg2, l, [1]))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('wa5', lg2, l, [1,1,1,1,1]))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('wa9', lg2, l, [1,1,1,1,1,1,1]))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('wa15', lg2, l, [1]*15))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('linear5', lg2, l, [1,2,3,2,1]))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('linear9', lg2, l, [1,2,3,4,5,4,3,2,1]))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('linear15', lg2, l, [1,2,3,4,5,6,7,8,7,6,5,4,3,2,1]))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('quad5', lg2, l, [1,3,9,3,1]))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('quad9', lg2, l, [1,3,9,27,81,27,9,3,1]))\n",
    "    metrics.update(precision_recall_f1_roc_convolve('quad15', lg2, l, [1,3,9,27,81,243,729,2187,729,243,81,27,9,3,1]))\n",
    "    \n",
    "    logits_path = OUTPUT_DIR + '/Logits/'\n",
    "    if not os.path.isdir(logits_path):\n",
    "        os.mkdir(logits_path)\n",
    "    new_df = deepcopy(df_val)\n",
    "    new_df['Logits'] = [[i[1] for i in x] for x in list(logits)]\n",
    "    pickle.dump(new_df, open(logits_path + datetime.now().strftime(\"%H:%M:%S\"), 'wb'))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "727848a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f'../Outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "508d9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = OUTPUT_DIR + '/Checkpoints',\n",
    "    num_train_epochs = EPOCHS, # 10 epochs\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    warmup_steps = 1000,\n",
    "    learning_rate = LEARNING_RATE, # 2e-06\n",
    "    logging_dir = OUTPUT_DIR + '/Logs',\n",
    "    logging_steps = 200,\n",
    "    lr_scheduler_type=SCHEDULER, # cosine with restarts\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch', # evaluate at every epoch\n",
    "    gradient_accumulation_steps = BATCH_SIZE,\n",
    "#     fp16 = True,\n",
    "#     fp16_opt_level = '02',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4a81a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "def model_init():\n",
    "    model = AutoModelForTokenClassification.from_pretrained(PRETRAINED_MODEL, num_labels=NUM_CLASSES)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "907bedf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af1ed3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='15690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   11/15690 01:05 < 31:29:55, 0.14 it/s, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dr-bert-env/lib/python3.9/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dr-bert-env/lib/python3.9/site-packages/transformers/trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1838\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1841\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1843\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1844\u001b[0m ):\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1846\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/dr-bert-env/lib/python3.9/site-packages/transformers/trainer.py:2704\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2702\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2704\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.conda/envs/dr-bert-env/lib/python3.9/site-packages/accelerate/accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dr-bert-env/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dr-bert-env/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cbf2960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "# packages in environment at /home/byw2/.conda/envs/dr-bert-env:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "pytorch                   2.0.1           cpu_py39hab5cca8_0  \n"
     ]
    }
   ],
   "source": [
    "!conda list torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d0a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr-bert-env [~/.conda/envs/dr-bert-env/]",
   "language": "python",
   "name": "conda_dr-bert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
